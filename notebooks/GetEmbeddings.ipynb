{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install webdataset ujson diffusers transformers"
      ],
      "metadata": {
        "id": "fH-U0vf-KSDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Yn78wn7VIE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import webdataset as wds\n",
        "import braceexpand\n",
        "from transformers import CLIPImageProcessor, CLIPModel, CLIPVisionModelWithProjection\n",
        "from diffusers import KandinskyV22PriorPipeline\n",
        "import pandas as pd\n",
        "import random\n",
        "import ujson\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import tqdm\n",
        "from functools import partialmethod\n",
        "_original_tqdm_init = tqdm.tqdm.__init__\n",
        "tqdm.tqdm.__init__ = partialmethod(tqdm.tqdm.__init__, disable=True)\n",
        "\n",
        "try:\n",
        "    import tqdm.notebook\n",
        "    _original_notebook_init = tqdm.notebook.tqdm.__init__\n",
        "    tqdm.notebook.tqdm.__init__ = partialmethod(tqdm.notebook.tqdm.__init__, disable=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def set_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "REPRODUCIBLE_SEED = 42\n",
        "set_everything(REPRODUCIBLE_SEED)\n",
        "CONFIG = {\n",
        "    \"dataset_size\": 4096, # no more if getting kandinsky embeds\n",
        "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"values_col\": \"embedding\",\n",
        "    \"clip_name\": \"openai/clip-vit-large-patch14\",\n",
        "    \"kandinsky_name\": \"kandinsky-community/kandinsky-2-2-prior\",\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class FileNameConfig:\n",
        "    model_name: str = None\n",
        "    values_col: str = None\n",
        "    mode: str = None\n",
        "    dir_path: str = \"../datasets/\" # set \"\" if running in google colab\n",
        "\n",
        "    @property\n",
        "    def filename(self):\n",
        "        if self.model_name is not None and self.mode is not None:\n",
        "            return self.dir_path + f\"sae_for_{self.model_name}_dataset_{self.mode}.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cc3mm_dataset(mode):\n",
        "\n",
        "  def get_cc3m_url(mode):\n",
        "    base_url = f\"hf://datasets/pixparse/cc3m-wds@main/cc3m-{mode}-\"\n",
        "    match mode:\n",
        "      case \"train\":\n",
        "        shard_range = \"{0000..0575}\"\n",
        "      case \"validation\":\n",
        "        shard_range = \"{0000..015}\"\n",
        "\n",
        "    return f\"{base_url}{shard_range}.tar\"\n",
        "\n",
        "  def get_dataset_from_url(url):\n",
        "    dataset = (\n",
        "        wds.WebDataset(url, shardshuffle=False)\n",
        "        .decode(\"pil\")\n",
        "        .rename(image=\"jpg\", caption=\"txt\")\n",
        "        .to_tuple(\"image\", \"caption\")\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "  return get_dataset_from_url(\n",
        "      get_cc3m_url(mode)\n",
        "  )\n",
        "\n",
        "def create_clip_processor(config):\n",
        "  device = config[\"device\"]\n",
        "  model_name = config[\"clip_name\"]\n",
        "\n",
        "  processor = CLIPImageProcessor.from_pretrained(\n",
        "      model_name,\n",
        "      use_fast=True,\n",
        "  )\n",
        "\n",
        "  model = CLIPModel.from_pretrained(\n",
        "      model_name\n",
        "  )\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def compute_numpy_clip_embeddings(image):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.get_image_features(**inputs)\n",
        "    return outputs.squeeze(0).cpu().numpy()\n",
        "\n",
        "  return compute_numpy_clip_embeddings\n",
        "\n",
        "def create_kandinsky_processor(config):\n",
        "  device = config[\"device\"]\n",
        "  model_name=config[\"kandinsky_name\"]\n",
        "\n",
        "  image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "    model_name,\n",
        "    subfolder='image_encoder'\n",
        "  ).half().to(device)\n",
        "\n",
        "  prior = KandinskyV22PriorPipeline.from_pretrained(\n",
        "    model_name,\n",
        "    image_encoder=image_encoder,\n",
        "    torch_dtype=torch.float16\n",
        "  ).to(device)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def compute_numpy_kandisky_embeddings(image):\n",
        "    embed = prior.interpolate([image], [1]).image_embeds[0]\n",
        "    return embed.cpu().numpy()\n",
        "\n",
        "  return compute_numpy_kandisky_embeddings\n",
        "\n",
        "def get_sae_dataset(dataset, config, embedding_computer):\n",
        "  limit = config[\"dataset_size\"]\n",
        "  values_col = config[\"values_col\"]\n",
        "\n",
        "  new_dataset_for_sae = {}\n",
        "  activates = []\n",
        "  for i, (image, text) in enumerate(dataset):\n",
        "      activates.append(\n",
        "          embedding_computer(image)\n",
        "      )\n",
        "      if i == limit-1:\n",
        "        break\n",
        "\n",
        "  new_dataset_for_sae[values_col] = [arr.tolist() for arr in activates]\n",
        "  return new_dataset_for_sae\n",
        "\n",
        "def save_sae_dataset(sae_dataset, config: FileNameConfig) -> None:\n",
        "    with open(config.filename, \"w\", encoding='utf-8') as df_file:\n",
        "        ujson.dump(sae_dataset, df_file)\n",
        "\n",
        "def inspect_sae_dataset(config: FileNameConfig) -> Dict[str, Any]:\n",
        "    with open(config.filename, \"r\", encoding='utf-8') as df_file:\n",
        "        df = ujson.load(df_file)\n",
        "    print(f\"{config.model_name} - {config.mode} mode\")\n",
        "    if config.values_col in df:\n",
        "        values = df[config.values_col]\n",
        "        print(f\"Keys: {list(df.keys())}\")\n",
        "        print(f\"Rows: {len(values)}\")\n",
        "        if values and hasattr(values[0], '__len__'):\n",
        "            print(f\"Columns: {len(values[0])}\")\n",
        "    print(\"\\n\")\n",
        "    return df\n",
        "\n",
        "def clear_memory():\n",
        "  gc.collect()\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "oHe_yevEG9xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROCESSORS = {\n",
        "    # \"clip\": create_clip_processor(CONFIG),\n",
        "    # \"kandinsky\": create_kandinsky_processor(CONFIG)\n",
        "} # choose the one(-s) you need\n",
        "\n",
        "clear_memory()\n",
        "SavingConf = FileNameConfig(values_col=CONFIG[\"values_col\"])"
      ],
      "metadata": {
        "id": "6tXBm2nrSUwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mode in [\"train\", \"validation\"]:\n",
        "  SavingConf.mode = mode\n",
        "\n",
        "  dataset = get_cc3mm_dataset(mode)\n",
        "  for model_name, processor in PROCESSORS.items():\n",
        "    SavingConf.model_name = model_name\n",
        "\n",
        "    sae_dataset = get_sae_dataset(dataset, CONFIG, processor)\n",
        "\n",
        "    save_sae_dataset(sae_dataset, SavingConf)\n",
        "    inspect_sae_dataset(SavingConf)\n",
        "\n",
        "    del sae_dataset\n",
        "    clear_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo9PuYP1MJUr",
        "outputId": "976fca15-4035-47b6-dd15-8775ef640df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kandinsky - train mode\n",
            "Keys: ['embedding']\n",
            "Rows: 4096\n",
            "Columns: 1280\n",
            "\n",
            "\n",
            "kandinsky - validation mode\n",
            "Keys: ['embedding']\n",
            "Rows: 4096\n",
            "Columns: 1280\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}